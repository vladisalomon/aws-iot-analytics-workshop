{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Smart Home Sensor Analysis</h1>\n",
    "<p>The ‘Household Power Consumption‘ dataset is a multivariate time series dataset that describes the electricity consumption for a single household for last few months. The  dataset is modeled after household consumption dataset available here - \n",
    "https://archive.ics.uci.edu/ml/datasets/individual+household+electric+power+consumption\n",
    "\n",
    "\n",
    "It is a multivariate series comprised of seven variables (besides the date and time); they are:\n",
    "\n",
    "<b>global_active_power:</b> The total active power consumed by the household (kilowatts).<br>\n",
    "<b>global_reactive_power:</b> The total reactive power consumed by the household (kilowatts).<br>\n",
    "<b>voltage:</b> Average voltage (volts).<br>\n",
    "<b>global_intensity:</b> Average current intensity (amps).<br>\n",
    "<b>sub_metering_1:</b> Active energy for kitchen (watt-hours of active energy).<br>\n",
    "<b>sub_metering_2:</b> Active energy for laundry (watt-hours of active energy).<br>\n",
    "<b>sub_metering_3:</b> Active energy for climate control systems (watt-hours of active energy).<br>\n",
    "\n",
    "<p> In the following section, we will analyze and predict hourly power consumption using DeepAR on SageMaker. The purpose of this exercise is to demonstrate Sagemaker integration with IoT Analytics and not to focus on training the right model to generate accurate prediction.\n",
    "<p>For more information see the DeepAR [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html) or [paper](https://arxiv.org/abs/1704.04110), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "from dateutil.parser import parse\n",
    "import json\n",
    "from random import shuffle\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntSlider, FloatSlider, Checkbox\n",
    "\n",
    "from sagemaker import get_execution_role\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, we can override the default values for the following:\n",
    "- The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = sagemaker.Session().default_bucket()  # replace with an existing bucket if needed\n",
    "s3_prefix = 'iot-analytics-demo-notebook'    # prefix used for all data stored within the bucket\n",
    "\n",
    "role = get_execution_role()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "s3_output_path = \"s3://{}/{}/output\".format(s3_bucket, s3_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we configure the container image to be used for the region that we are running in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = sagemaker.image_uris.retrieve(\"forecasting-deepar\", region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import  dataset from the IotAnalytics database and upload it to S3 to make it available for Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reading from the IotAnalytics database\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from numpy import isnan\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from numpy import isnan\n",
    "\n",
    "\n",
    "client = boto3.client('iotanalytics')\n",
    "dataset = \"batchdataset\"\n",
    "data_location = client.get_dataset_content(datasetName = dataset)['entries'][0]['dataURI']\n",
    "\n",
    "# Function to convert date columns into a single timestamp field\n",
    "#def parse(x):\n",
    "#    return datetime.strptime(x, '%Y %m %d %H')\n",
    "def parse(x):\n",
    "    t= pd.to_datetime(str(x)) \n",
    "    timestring = t.strftime('%Y.%m.%d %H:%M:%S')\n",
    "    return timestring\n",
    "\n",
    "def fill_missing(values):\n",
    "    one_day = 60 * 24\n",
    "    for row in range(values.shape[0]):\n",
    "        for col in range(values.shape[1]):\n",
    "            if np.isnan(values[row, col]):\n",
    "                values[row, col] = values[row - one_day, col]\n",
    " \n",
    "# load all data\n",
    "\n",
    "dataset = pd.read_csv(data_location,  header=0, low_memory=False, infer_datetime_format=True, date_parser = parse, index_col=['timestamp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####Examine the dataset\n",
      "dataset shape  (5000, 9)\n",
      "                     sub_metering_1  sub_metering_2  sub_metering_3  \\\n",
      "timestamp                                                             \n",
      "2021-02-10 21:48:31           46.48          100.85           12.82   \n",
      "2021-02-10 21:48:34           82.82           21.89           10.10   \n",
      "2021-02-10 21:48:37           73.98           95.29           17.10   \n",
      "2021-02-10 21:48:40           99.40           38.30           13.23   \n",
      "2021-02-10 21:48:43           21.17           74.86           12.11   \n",
      "\n",
      "                     global_active_power  global_reactive_power  voltage  \\\n",
      "timestamp                                                                  \n",
      "2021-02-10 21:48:31                 6.33                  17.38    24.71   \n",
      "2021-02-10 21:48:34                 4.84                  18.38   131.52   \n",
      "2021-02-10 21:48:37                 3.57                  32.47   149.69   \n",
      "2021-02-10 21:48:40                 7.21                   8.94   198.29   \n",
      "2021-02-10 21:48:43                 1.85                  25.34   123.10   \n",
      "\n",
      "                        cost  global_intensity                     __dt  \n",
      "timestamp                                                                \n",
      "2021-02-10 21:48:31  240.225               NaN  2021-02-10 00:00:00.000  \n",
      "2021-02-10 21:48:34  172.215               NaN  2021-02-10 00:00:00.000  \n",
      "2021-02-10 21:48:37  279.555               NaN  2021-02-10 00:00:00.000  \n",
      "2021-02-10 21:48:40  226.395               NaN  2021-02-10 00:00:00.000  \n",
      "2021-02-10 21:48:43  162.210               NaN  2021-02-10 00:00:00.000  \n"
     ]
    }
   ],
   "source": [
    "print(\"#####Examine the dataset\")\n",
    "print(\"dataset shape \",dataset.shape)\n",
    "print(dataset.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Explore the data</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "cor_cols = [\"global_active_power\",\"global_reactive_power\", \"global_intensity\",\"voltage\",\"sub_metering_1\",\"sub_metering_2\",\"sub_metering_3\",\"cost\"]\n",
    "\n",
    "\n",
    "# plot correlation matrix\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.matshow(dataset.loc[:, cor_cols].corr(), fignum=1)\n",
    "plt.xticks(range(len(cor_cols)), cor_cols, rotation=90)\n",
    "plt.yticks(range(len(cor_cols)), cor_cols)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Correlation Plot</b>\n",
    "<p>The correlation plot, we can observe that Sub Meters 1 and 2 show some correlation with each other. Sub meter 3 has less correlation with Sub meter 1 and 2. Since cost is derived by aggregating Sub Meter 1, 2 and 3, we will retain only cost column to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "##updating data to dataset\n",
    "analysis_cols = [\"global_active_power\",\"global_reactive_power\", \"global_intensity\",\"voltage\",\"cost\"]\n",
    "\n",
    "# select the value columns in the DataFrame to compare\n",
    "dataset= dataset.loc[:, analysis_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we load and parse the dataset and convert it to a collection of Pandas time series, which makes common time series operations such as indexing by time periods or resampling much easier. Here we want to forecast longer periods (one week) and resample the data to a granularity of every hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timeseries = dataset.shape[1]\n",
    "data_kw = dataset.resample('H').sum() \n",
    "\n",
    "timeseries = []\n",
    "for i in range(num_timeseries):\n",
    "    timeseries.append(np.trim_zeros(data_kw.iloc[:,i], trim='f'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test splits\n",
    "\n",
    "Often times one is interested in evaluating the model or tuning its hyperparameters by looking at error metrics on a hold-out test set. Here we split the available data into train and test sets for evaluating the trained model. For standard machine learning tasks such as classification and regression, one typically obtains this split by randomly separating examples into train and test sets. However, in forecasting it is important to do this train/test split based on time rather than by time series.\n",
    "\n",
    "In this example, we will reserve the last section of each of the time series for evalutation purpose and use only the first part as training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use 1 (changed 2 to 1) hour frequency for the time series\n",
    "freq = 'H' \n",
    "\n",
    "# we predict for 7 days\n",
    "prediction_length = 7 * 24  ##original 7*12\n",
    "\n",
    "# we also use 7 days as context length, this is the number of state updates accomplished before making predictions\n",
    "context_length = 7 * 24 ##original 7*12 due to 2H sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we assume that the minimum date is the first day sensors started recording the dataset. We split the dataset 70, 30 with 70% of the data retained for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Confirm the datasets\n",
      "startTrainDate, maxDate, traingSplit, splitDate 2018-07-04 00:00:00 2021-02-10 00:00:00 667 2020-05-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "dailyGroups = dataset.resample('D').sum() ## Gathering unique days for dataset split\n",
    "\n",
    "startTrainDate = dailyGroups.index.min()\n",
    "\n",
    "maxDate=dailyGroups.index.max()\n",
    "\n",
    "\n",
    "traingSplit=round(dailyGroups.shape[0]*.7)\n",
    "\n",
    "\n",
    "splitDate=dailyGroups.index[traingSplit]\n",
    "\n",
    "\n",
    "start_dataset = pd.Timestamp(startTrainDate, freq=freq)\n",
    "end_training = pd.Timestamp(splitDate, freq=freq)\n",
    "\n",
    "print(\"###### Confirm the datasets\")\n",
    "print(\"startTrainDate, maxDate, traingSplit, splitDate\", startTrainDate, maxDate, traingSplit, splitDate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DeepAR JSON input format represents each time series as a JSON object. In the simplest case each time series just consists of a start time stamp (``start``) and a list of values (``target``). For more complex cases, DeepAR also supports the fields ``dynamic_feat`` for time-series features and ``cat`` for categorical features, which we will use  later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "training_data = [\n",
    "    {\n",
    "        \"start\": str(start_dataset),\n",
    "        \"target\": ts[start_dataset:end_training - pd.Timedelta(1, unit=freq)].tolist()  # We use -1, because pandas indexing includes the upper bound \n",
    "    }\n",
    "    for ts in timeseries\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As test data, we will consider time series extending beyond the training range: these will be used for computing test scores, by using the trained model to forecast their trailing 7 days, and comparing predictions with actual values.\n",
    "To evaluate our model performance on more than one week, we generate test data that extends to 1, 2, 3, 4 weeks beyond the training range. This way we perform *rolling evaluation* of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_windows = 4\n",
    "\n",
    "test_data = [\n",
    "    {\n",
    "        \"start\": str(start_dataset),\n",
    "        \"target\": ts[start_dataset:end_training + pd.Timedelta(k * prediction_length, unit=freq)].tolist()\n",
    "    }\n",
    "    for k in range(1, num_test_windows + 1) \n",
    "    for ts in timeseries\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now write the dictionary to the `jsonlines` file format that DeepAR understands (it also supports gzipped jsonlines and parquet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, 'wb') as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59.6 ms, sys: 12.5 ms, total: 72.1 ms\n",
      "Wall time: 71.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "write_dicts_to_file(\"train.json\", training_data)\n",
    "write_dicts_to_file(\"test.json\", test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data files locally, let us copy them to S3 where DeepAR can access them. Depending on your connection, this may take a couple of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "def copy_to_s3(local_file, s3_path, override=True):\n",
    "    assert s3_path.startswith('s3://')\n",
    "    split = s3_path.split('/')\n",
    "    bucket = split[2]\n",
    "    path = '/'.join(split[3:])\n",
    "    buk = s3.Bucket(bucket)\n",
    "    \n",
    "    if len(list(buk.objects.filter(Prefix=path))) > 0:\n",
    "        if not override:\n",
    "            print('File s3://{}/{} already exists.\\nSet override to upload anyway.\\n'.format(s3_bucket, s3_path))\n",
    "            return\n",
    "        else:\n",
    "            print('Overwriting existing file')\n",
    "    with open(local_file, 'rb') as data:\n",
    "        print('Uploading file to {}'.format(s3_path))\n",
    "        buk.put_object(Key=path, Body=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting existing file\n",
      "Uploading file to s3://sagemaker-us-west-2-246290524667/iot-analytics-demo-notebook/output/train/train.json\n",
      "Overwriting existing file\n",
      "Uploading file to s3://sagemaker-us-west-2-246290524667/iot-analytics-demo-notebook/output/test/test.json\n",
      "CPU times: user 45.9 ms, sys: 16.8 ms, total: 62.7 ms\n",
      "Wall time: 493 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "copy_to_s3(\"train.json\", s3_output_path + \"/train/train.json\")\n",
    "copy_to_s3(\"test.json\", s3_output_path + \"/test/test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look to what we just wrote to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"start\": \"2018-07-04 00:00:00\", \"target\": [0.25, 2.8, 0.89, 0.0, 12.59, 0.42, 0.0, 0.0, 0.0, 0.0, 0...\n"
     ]
    }
   ],
   "source": [
    "s3filesystem = s3fs.S3FileSystem()\n",
    "with s3filesystem.open(s3_output_path + \"/train/train.json\", 'rb') as fp:\n",
    "    print(fp.readline().decode(\"utf-8\")[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are all set with our dataset processing, we can now call DeepAR to train a model and generate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model\n",
    "\n",
    "Here we define the estimator that will launch the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_uri=image_name,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c4.2xlarge',\n",
    "    base_job_name='deepar-electricity-demo',\n",
    "    output_path=s3_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to set the hyperparameters for the training job. For example frequency of the time series used, number of data points the model will look at in the past, number of predicted data points. The other hyperparameters concern the model to train (number of layers, number of cells per layer, likelihood function) and the training options (number of epochs, batch size, learning rate...). We use default parameters for every optional parameter in this case (you can always use [Sagemaker Automated Model Tuning](https://aws.amazon.com/blogs/aws/sagemaker-automatic-model-tuning/) to tune them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"40\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"1E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to launch the training job. SageMaker will start an EC2 instance, download the data from S3, start training the model and save the trained model.\n",
    "\n",
    "If you provide the `test` data channel as we do in this example, DeepAR will also calculate accuracy metrics for the trained model on this test. This is done by predicting the last `prediction_length` points of each time-series in the test set and comparing this to the actual value of the time-series. \n",
    "\n",
    "**Note:** the next cell may take a few minutes to complete, depending on data size, model complexity, training options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-11 18:29:46 Starting - Starting the training job...\n",
      "2021-02-11 18:29:48 Starting - Launching requested ML instancesProfilerReport-1613068186: InProgress\n",
      "......\n",
      "2021-02-11 18:31:16 Starting - Preparing the instances for training.........\n",
      "2021-02-11 18:32:39 Downloading - Downloading input data\n",
      "2021-02-11 18:32:39 Training - Downloading the training image...\n",
      "2021-02-11 18:33:15 Training - Training image download completed. Training in progress.\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:10 INFO 139671722272576] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:10 INFO 139671722272576] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'1E-4', u'prediction_length': u'168', u'epochs': u'40', u'time_freq': u'H', u'context_length': u'168', u'mini_batch_size': u'64', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:10 INFO 139671722272576] Final configuration: {u'dropout_rate': u'0.10', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'1E-4', u'num_layers': u'2', u'epochs': u'40', u'embedding_dimension': u'10', u'num_cells': u'40', u'_num_kv_servers': u'auto', u'mini_batch_size': u'64', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'168', u'time_freq': u'H', u'context_length': u'168', u'_kvstore': u'auto', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:10 INFO 139671722272576] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:10 INFO 139671722272576] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:10 INFO 139671722272576] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:10 INFO 139671722272576] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:10 INFO 139671722272576] Training set statistics:\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:10 INFO 139671722272576] Real time series\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:10 INFO 139671722272576] number of time series: 5\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:10 INFO 139671722272576] number of observations: 79955\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:10 INFO 139671722272576] mean target length: 15991\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:10 INFO 139671722272576] min/mean/max target: 0.0/12.6995093493/3411.72998047\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:10 INFO 139671722272576] mean abs(target): 12.6995093493\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:10 INFO 139671722272576] contains missing values: no\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:10 INFO 139671722272576] Small number of time series. Doing 128 passes over dataset with prob 1.0 per epoch.\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:10 INFO 139671722272576] Test set statistics:\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:10 INFO 139671722272576] Real time series\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:10 INFO 139671722272576] number of time series: 20\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:10 INFO 139671722272576] number of observations: 328240\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:10 INFO 139671722272576] mean target length: 16412\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:11 INFO 139671722272576] min/mean/max target: 0.0/12.3737420183/3411.72998047\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:11 INFO 139671722272576] mean abs(target): 12.3737420183\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:11 INFO 139671722272576] contains missing values: no\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:11 INFO 139671722272576] nvidia-smi took: 0.0252678394318 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:11 INFO 139671722272576] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:11 INFO 139671722272576] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 10220.49617767334, \"sum\": 10220.49617767334, \"min\": 10220.49617767334}}, \"EndTime\": 1613068401.251468, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068391.03015}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:21 INFO 139671722272576] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 11709.651947021484, \"sum\": 11709.651947021484, \"min\": 11709.651947021484}}, \"EndTime\": 1613068402.73991, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068401.251672}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:24 INFO 139671722272576] Epoch[0] Batch[0] avg_epoch_loss=3.616124\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:24 INFO 139671722272576] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=3.61612391472\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:26 INFO 139671722272576] Epoch[0] Batch[5] avg_epoch_loss=3.492026\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:26 INFO 139671722272576] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=3.49202613036\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:26 INFO 139671722272576] Epoch[0] Batch [5]#011Speed: 157.24 samples/sec#011loss=3.492026\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:28 INFO 139671722272576] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 40, \"sum\": 40.0, \"min\": 40}, \"update.time\": {\"count\": 1, \"max\": 5743.026971817017, \"sum\": 5743.026971817017, \"min\": 5743.026971817017}}, \"EndTime\": 1613068408.483139, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068402.740021}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:28 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=110.740046865 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:28 INFO 139671722272576] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:28 INFO 139671722272576] #quality_metric: host=algo-1, epoch=0, train loss <loss>=3.46160976887\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:28 INFO 139671722272576] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:28 INFO 139671722272576] Saved checkpoint to \"/opt/ml/model/state_241e8d12-4f47-4ad2-a6a7-9322fd595005-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 188.59505653381348, \"sum\": 188.59505653381348, \"min\": 188.59505653381348}}, \"EndTime\": 1613068408.672401, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068408.483248}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:30 INFO 139671722272576] Epoch[1] Batch[0] avg_epoch_loss=3.220997\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:30 INFO 139671722272576] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=3.22099661827\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:32 INFO 139671722272576] Epoch[1] Batch[5] avg_epoch_loss=3.344999\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:32 INFO 139671722272576] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=3.34499915441\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:32 INFO 139671722272576] Epoch[1] Batch [5]#011Speed: 156.84 samples/sec#011loss=3.344999\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:34 INFO 139671722272576] Epoch[1] Batch[10] avg_epoch_loss=3.280308\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:34 INFO 139671722272576] #quality_metric: host=algo-1, epoch=1, batch=10 train loss <loss>=3.20267934799\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:34 INFO 139671722272576] Epoch[1] Batch [10]#011Speed: 152.60 samples/sec#011loss=3.202679\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:34 INFO 139671722272576] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5798.33197593689, \"sum\": 5798.33197593689, \"min\": 5798.33197593689}}, \"EndTime\": 1613068414.470866, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068408.672471}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:34 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=113.651122806 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:34 INFO 139671722272576] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:34 INFO 139671722272576] #quality_metric: host=algo-1, epoch=1, train loss <loss>=3.28030833331\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:34 INFO 139671722272576] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:34 INFO 139671722272576] Saved checkpoint to \"/opt/ml/model/state_f6771fba-1dba-47bd-8ca6-fcf0d9200104-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 124.55010414123535, \"sum\": 124.55010414123535, \"min\": 124.55010414123535}}, \"EndTime\": 1613068414.59595, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068414.470945}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:36 INFO 139671722272576] Epoch[2] Batch[0] avg_epoch_loss=3.130924\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:36 INFO 139671722272576] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=3.13092446327\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:38 INFO 139671722272576] Epoch[2] Batch[5] avg_epoch_loss=3.122529\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:38 INFO 139671722272576] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=3.12252867222\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:38 INFO 139671722272576] Epoch[2] Batch [5]#011Speed: 154.52 samples/sec#011loss=3.122529\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:40 INFO 139671722272576] Epoch[2] Batch[10] avg_epoch_loss=3.085924\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:40 INFO 139671722272576] #quality_metric: host=algo-1, epoch=2, batch=10 train loss <loss>=3.04199872017\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:40 INFO 139671722272576] Epoch[2] Batch [10]#011Speed: 152.58 samples/sec#011loss=3.041999\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:40 INFO 139671722272576] processed a total of 683 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5761.539936065674, \"sum\": 5761.539936065674, \"min\": 5761.539936065674}}, \"EndTime\": 1613068420.357634, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068414.596031}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:40 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=118.5423708 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:40 INFO 139671722272576] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:40 INFO 139671722272576] #quality_metric: host=algo-1, epoch=2, train loss <loss>=3.08592414856\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:40 INFO 139671722272576] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:40 INFO 139671722272576] Saved checkpoint to \"/opt/ml/model/state_72d697b7-7458-43fc-b9d8-2f3e62a4cf30-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 121.81901931762695, \"sum\": 121.81901931762695, \"min\": 121.81901931762695}}, \"EndTime\": 1613068420.48001, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068420.357711}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:42 INFO 139671722272576] Epoch[3] Batch[0] avg_epoch_loss=3.118204\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:42 INFO 139671722272576] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=3.11820435524\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:44 INFO 139671722272576] Epoch[3] Batch[5] avg_epoch_loss=2.944725\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:44 INFO 139671722272576] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=2.94472483794\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:44 INFO 139671722272576] Epoch[3] Batch [5]#011Speed: 157.02 samples/sec#011loss=2.944725\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:46 INFO 139671722272576] Epoch[3] Batch[10] avg_epoch_loss=2.916888\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:46 INFO 139671722272576] #quality_metric: host=algo-1, epoch=3, batch=10 train loss <loss>=2.88348369598\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:46 INFO 139671722272576] Epoch[3] Batch [10]#011Speed: 154.03 samples/sec#011loss=2.883484\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:46 INFO 139671722272576] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5697.158098220825, \"sum\": 5697.158098220825, \"min\": 5697.158098220825}}, \"EndTime\": 1613068426.177286, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068420.480073}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:46 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=118.302296134 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:46 INFO 139671722272576] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:46 INFO 139671722272576] #quality_metric: host=algo-1, epoch=3, train loss <loss>=2.91688795523\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:46 INFO 139671722272576] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:46 INFO 139671722272576] Saved checkpoint to \"/opt/ml/model/state_814ce763-973c-4a2d-98e2-c1fb454d5994-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 119.79007720947266, \"sum\": 119.79007720947266, \"min\": 119.79007720947266}}, \"EndTime\": 1613068426.297621, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068426.177363}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:47 INFO 139671722272576] Epoch[4] Batch[0] avg_epoch_loss=2.875440\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:47 INFO 139671722272576] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=2.87544035912\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:49 INFO 139671722272576] Epoch[4] Batch[5] avg_epoch_loss=2.775329\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:49 INFO 139671722272576] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=2.7753289938\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:49 INFO 139671722272576] Epoch[4] Batch [5]#011Speed: 157.66 samples/sec#011loss=2.775329\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:51 INFO 139671722272576] Epoch[4] Batch[10] avg_epoch_loss=2.711291\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:51 INFO 139671722272576] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=2.63444623947\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:51 INFO 139671722272576] Epoch[4] Batch [10]#011Speed: 150.82 samples/sec#011loss=2.634446\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:51 INFO 139671722272576] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5693.886995315552, \"sum\": 5693.886995315552, \"min\": 5693.886995315552}}, \"EndTime\": 1613068431.991648, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068426.297701}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:51 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=113.979928235 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:51 INFO 139671722272576] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:51 INFO 139671722272576] #quality_metric: host=algo-1, epoch=4, train loss <loss>=2.71129137819\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:51 INFO 139671722272576] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:52 INFO 139671722272576] Saved checkpoint to \"/opt/ml/model/state_59793bc1-8aa2-4a0b-9ca3-806939c4f827-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 118.45088005065918, \"sum\": 118.45088005065918, \"min\": 118.45088005065918}}, \"EndTime\": 1613068432.110657, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068431.991711}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:53 INFO 139671722272576] Epoch[5] Batch[0] avg_epoch_loss=2.539250\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:53 INFO 139671722272576] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=2.53925013542\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:55 INFO 139671722272576] Epoch[5] Batch[5] avg_epoch_loss=2.563316\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:55 INFO 139671722272576] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=2.56331586838\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:55 INFO 139671722272576] Epoch[5] Batch [5]#011Speed: 151.50 samples/sec#011loss=2.563316\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:57 INFO 139671722272576] Epoch[5] Batch[10] avg_epoch_loss=2.516748\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:57 INFO 139671722272576] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=2.46086554527\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:57 INFO 139671722272576] Epoch[5] Batch [10]#011Speed: 151.30 samples/sec#011loss=2.460866\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:57 INFO 139671722272576] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5759.546995162964, \"sum\": 5759.546995162964, \"min\": 5759.546995162964}}, \"EndTime\": 1613068437.870362, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068432.110756}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:57 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=112.68024361 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:57 INFO 139671722272576] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:57 INFO 139671722272576] #quality_metric: host=algo-1, epoch=5, train loss <loss>=2.51674753969\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:57 INFO 139671722272576] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:58 INFO 139671722272576] Saved checkpoint to \"/opt/ml/model/state_8c41001d-7103-4eeb-8939-eaac5ea425b4-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 133.4679126739502, \"sum\": 133.4679126739502, \"min\": 133.4679126739502}}, \"EndTime\": 1613068438.00445, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068437.87044}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:59 INFO 139671722272576] Epoch[6] Batch[0] avg_epoch_loss=2.321210\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:33:59 INFO 139671722272576] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=2.32120990753\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:01 INFO 139671722272576] Epoch[6] Batch[5] avg_epoch_loss=2.393430\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:01 INFO 139671722272576] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=2.39342987537\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:01 INFO 139671722272576] Epoch[6] Batch [5]#011Speed: 149.53 samples/sec#011loss=2.393430\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:03 INFO 139671722272576] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5676.0759353637695, \"sum\": 5676.0759353637695, \"min\": 5676.0759353637695}}, \"EndTime\": 1613068443.680679, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068438.004527}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:03 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=111.51828971 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:03 INFO 139671722272576] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:03 INFO 139671722272576] #quality_metric: host=algo-1, epoch=6, train loss <loss>=2.38223206997\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:03 INFO 139671722272576] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:03 INFO 139671722272576] Saved checkpoint to \"/opt/ml/model/state_1326ca58-c5d3-4281-a2d7-84604873a767-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 139.19997215270996, \"sum\": 139.19997215270996, \"min\": 139.19997215270996}}, \"EndTime\": 1613068443.820605, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068443.680762}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:05 INFO 139671722272576] Epoch[7] Batch[0] avg_epoch_loss=2.210127\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:05 INFO 139671722272576] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=2.21012735367\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:07 INFO 139671722272576] Epoch[7] Batch[5] avg_epoch_loss=2.240456\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:07 INFO 139671722272576] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=2.24045638243\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:07 INFO 139671722272576] Epoch[7] Batch [5]#011Speed: 154.76 samples/sec#011loss=2.240456\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:09 INFO 139671722272576] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5385.5040073394775, \"sum\": 5385.5040073394775, \"min\": 5385.5040073394775}}, \"EndTime\": 1613068449.206248, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068443.820681}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:09 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=114.564235926 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:09 INFO 139671722272576] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:09 INFO 139671722272576] #quality_metric: host=algo-1, epoch=7, train loss <loss>=2.17303346395\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:09 INFO 139671722272576] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:09 INFO 139671722272576] Saved checkpoint to \"/opt/ml/model/state_c7e58fd9-7f08-457b-842b-c6635cdd13d7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 130.1860809326172, \"sum\": 130.1860809326172, \"min\": 130.1860809326172}}, \"EndTime\": 1613068449.337067, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068449.206331}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:10 INFO 139671722272576] Epoch[8] Batch[0] avg_epoch_loss=1.984986\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:10 INFO 139671722272576] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=1.9849858284\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:12 INFO 139671722272576] Epoch[8] Batch[5] avg_epoch_loss=2.018535\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:12 INFO 139671722272576] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=2.01853497823\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:12 INFO 139671722272576] Epoch[8] Batch [5]#011Speed: 153.39 samples/sec#011loss=2.018535\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:14 INFO 139671722272576] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5336.946964263916, \"sum\": 5336.946964263916, \"min\": 5336.946964263916}}, \"EndTime\": 1613068454.674156, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068449.337141}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:14 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=116.168630382 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:14 INFO 139671722272576] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:14 INFO 139671722272576] #quality_metric: host=algo-1, epoch=8, train loss <loss>=1.97386713028\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:14 INFO 139671722272576] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:14 INFO 139671722272576] Saved checkpoint to \"/opt/ml/model/state_8bc25d27-7205-4868-b81e-ac5eb22b3123-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 121.50287628173828, \"sum\": 121.50287628173828, \"min\": 121.50287628173828}}, \"EndTime\": 1613068454.796305, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068454.674239}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:16 INFO 139671722272576] Epoch[9] Batch[0] avg_epoch_loss=1.980687\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:16 INFO 139671722272576] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=1.98068726063\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:18 INFO 139671722272576] Epoch[9] Batch[5] avg_epoch_loss=1.886384\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:18 INFO 139671722272576] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=1.88638367256\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:18 INFO 139671722272576] Epoch[9] Batch [5]#011Speed: 150.64 samples/sec#011loss=1.886384\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:20 INFO 139671722272576] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5370.588064193726, \"sum\": 5370.588064193726, \"min\": 5370.588064193726}}, \"EndTime\": 1613068460.167032, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068454.796378}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:20 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=118.047682996 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:20 INFO 139671722272576] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:20 INFO 139671722272576] #quality_metric: host=algo-1, epoch=9, train loss <loss>=1.8830045104\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:20 INFO 139671722272576] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:20 INFO 139671722272576] Saved checkpoint to \"/opt/ml/model/state_f52346c3-2466-4cd2-a5a0-85f0cd9bdd62-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 126.44696235656738, \"sum\": 126.44696235656738, \"min\": 126.44696235656738}}, \"EndTime\": 1613068460.294077, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068460.167117}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:21 INFO 139671722272576] Epoch[10] Batch[0] avg_epoch_loss=1.548575\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:21 INFO 139671722272576] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=1.54857456684\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:23 INFO 139671722272576] Epoch[10] Batch[5] avg_epoch_loss=1.686617\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:23 INFO 139671722272576] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=1.68661747376\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:23 INFO 139671722272576] Epoch[10] Batch [5]#011Speed: 154.04 samples/sec#011loss=1.686617\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:25 INFO 139671722272576] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5367.789030075073, \"sum\": 5367.789030075073, \"min\": 5367.789030075073}}, \"EndTime\": 1613068465.662011, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068460.294156}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:25 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=115.314875456 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:25 INFO 139671722272576] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:25 INFO 139671722272576] #quality_metric: host=algo-1, epoch=10, train loss <loss>=1.6979372859\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:25 INFO 139671722272576] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:25 INFO 139671722272576] Saved checkpoint to \"/opt/ml/model/state_45cfda4e-3630-43af-b2b3-31a907d12345-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 123.9469051361084, \"sum\": 123.9469051361084, \"min\": 123.9469051361084}}, \"EndTime\": 1613068465.786586, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068465.662095}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:27 INFO 139671722272576] Epoch[11] Batch[0] avg_epoch_loss=1.617543\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:27 INFO 139671722272576] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=1.61754345894\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:29 INFO 139671722272576] Epoch[11] Batch[5] avg_epoch_loss=1.544042\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:29 INFO 139671722272576] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=1.54404165347\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:29 INFO 139671722272576] Epoch[11] Batch [5]#011Speed: 153.62 samples/sec#011loss=1.544042\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:31 INFO 139671722272576] Epoch[11] Batch[10] avg_epoch_loss=1.467571\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:31 INFO 139671722272576] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=1.37580635548\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:31 INFO 139671722272576] Epoch[11] Batch [10]#011Speed: 150.83 samples/sec#011loss=1.375806\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:31 INFO 139671722272576] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5774.300098419189, \"sum\": 5774.300098419189, \"min\": 5774.300098419189}}, \"EndTime\": 1613068471.561046, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068465.786664}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:31 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=111.699280508 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:31 INFO 139671722272576] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:31 INFO 139671722272576] #quality_metric: host=algo-1, epoch=11, train loss <loss>=1.46757106348\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:31 INFO 139671722272576] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:31 INFO 139671722272576] Saved checkpoint to \"/opt/ml/model/state_5fe0da84-a749-449e-87bd-694bf650bf57-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 127.49409675598145, \"sum\": 127.49409675598145, \"min\": 127.49409675598145}}, \"EndTime\": 1613068471.689122, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068471.561125}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:33 INFO 139671722272576] Epoch[12] Batch[0] avg_epoch_loss=1.473003\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:33 INFO 139671722272576] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=1.47300338745\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:35 INFO 139671722272576] Epoch[12] Batch[5] avg_epoch_loss=1.357960\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:35 INFO 139671722272576] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=1.35795970758\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:35 INFO 139671722272576] Epoch[12] Batch [5]#011Speed: 156.27 samples/sec#011loss=1.357960\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:36 INFO 139671722272576] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5284.7900390625, \"sum\": 5284.7900390625, \"min\": 5284.7900390625}}, \"EndTime\": 1613068476.974059, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068471.689201}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:36 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=116.369011872 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:36 INFO 139671722272576] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:36 INFO 139671722272576] #quality_metric: host=algo-1, epoch=12, train loss <loss>=1.41015257835\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:36 INFO 139671722272576] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:37 INFO 139671722272576] Saved checkpoint to \"/opt/ml/model/state_56c4a273-cacc-406a-8b14-d4ab6ba5c8da-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 123.72183799743652, \"sum\": 123.72183799743652, \"min\": 123.72183799743652}}, \"EndTime\": 1613068477.098406, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068476.974142}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:38 INFO 139671722272576] Epoch[13] Batch[0] avg_epoch_loss=1.527997\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:38 INFO 139671722272576] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=1.52799654007\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:40 INFO 139671722272576] Epoch[13] Batch[5] avg_epoch_loss=1.330457\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:40 INFO 139671722272576] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=1.33045697212\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:40 INFO 139671722272576] Epoch[13] Batch [5]#011Speed: 152.78 samples/sec#011loss=1.330457\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:42 INFO 139671722272576] Epoch[13] Batch[10] avg_epoch_loss=1.245160\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:42 INFO 139671722272576] #quality_metric: host=algo-1, epoch=13, batch=10 train loss <loss>=1.14280428886\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:42 INFO 139671722272576] Epoch[13] Batch [10]#011Speed: 155.17 samples/sec#011loss=1.142804\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:42 INFO 139671722272576] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5760.931015014648, \"sum\": 5760.931015014648, \"min\": 5760.931015014648}}, \"EndTime\": 1613068482.859476, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068477.09848}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:42 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=117.339803898 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:42 INFO 139671722272576] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:42 INFO 139671722272576] #quality_metric: host=algo-1, epoch=13, train loss <loss>=1.24516029791\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:42 INFO 139671722272576] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:42 INFO 139671722272576] Saved checkpoint to \"/opt/ml/model/state_8028351a-7814-4f6d-8184-5020ad134bda-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 114.09187316894531, \"sum\": 114.09187316894531, \"min\": 114.09187316894531}}, \"EndTime\": 1613068482.974134, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068482.859554}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:44 INFO 139671722272576] Epoch[14] Batch[0] avg_epoch_loss=0.968669\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:44 INFO 139671722272576] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=0.968669116497\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:46 INFO 139671722272576] Epoch[14] Batch[5] avg_epoch_loss=1.002775\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:46 INFO 139671722272576] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=1.00277541081\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:46 INFO 139671722272576] Epoch[14] Batch [5]#011Speed: 158.09 samples/sec#011loss=1.002775\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:48 INFO 139671722272576] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5255.403995513916, \"sum\": 5255.403995513916, \"min\": 5255.403995513916}}, \"EndTime\": 1613068488.229665, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068482.974192}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:48 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=121.015802096 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:48 INFO 139671722272576] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:48 INFO 139671722272576] #quality_metric: host=algo-1, epoch=14, train loss <loss>=1.03191683292\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:48 INFO 139671722272576] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:48 INFO 139671722272576] Saved checkpoint to \"/opt/ml/model/state_49c27c0a-025d-4962-a496-ece4f1f19e43-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 114.10212516784668, \"sum\": 114.10212516784668, \"min\": 114.10212516784668}}, \"EndTime\": 1613068488.344455, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068488.229732}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:49 INFO 139671722272576] Epoch[15] Batch[0] avg_epoch_loss=1.105199\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:49 INFO 139671722272576] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=1.10519874096\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:51 INFO 139671722272576] Epoch[15] Batch[5] avg_epoch_loss=0.993687\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:51 INFO 139671722272576] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=0.993687361479\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:51 INFO 139671722272576] Epoch[15] Batch [5]#011Speed: 156.71 samples/sec#011loss=0.993687\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:53 INFO 139671722272576] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5242.549896240234, \"sum\": 5242.549896240234, \"min\": 5242.549896240234}}, \"EndTime\": 1613068493.587149, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068488.344533}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:53 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=121.502901425 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:53 INFO 139671722272576] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:53 INFO 139671722272576] #quality_metric: host=algo-1, epoch=15, train loss <loss>=0.927776932716\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:53 INFO 139671722272576] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:53 INFO 139671722272576] Saved checkpoint to \"/opt/ml/model/state_14736532-9f1b-4977-9ecc-df6ea89e42c3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 114.17198181152344, \"sum\": 114.17198181152344, \"min\": 114.17198181152344}}, \"EndTime\": 1613068493.701966, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068493.587232}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:55 INFO 139671722272576] Epoch[16] Batch[0] avg_epoch_loss=0.832034\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:55 INFO 139671722272576] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=0.832034111023\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:57 INFO 139671722272576] Epoch[16] Batch[5] avg_epoch_loss=0.773740\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:57 INFO 139671722272576] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=0.773740356167\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:57 INFO 139671722272576] Epoch[16] Batch [5]#011Speed: 157.67 samples/sec#011loss=0.773740\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:59 INFO 139671722272576] Epoch[16] Batch[10] avg_epoch_loss=0.704160\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:59 INFO 139671722272576] #quality_metric: host=algo-1, epoch=16, batch=10 train loss <loss>=0.620663315058\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:59 INFO 139671722272576] Epoch[16] Batch [10]#011Speed: 150.43 samples/sec#011loss=0.620663\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:59 INFO 139671722272576] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5751.286029815674, \"sum\": 5751.286029815674, \"min\": 5751.286029815674}}, \"EndTime\": 1613068499.453396, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068493.702043}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:59 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=114.233037479 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:59 INFO 139671722272576] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:59 INFO 139671722272576] #quality_metric: host=algo-1, epoch=16, train loss <loss>=0.704159882936\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:59 INFO 139671722272576] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:34:59 INFO 139671722272576] Saved checkpoint to \"/opt/ml/model/state_941be745-b24c-401c-9e8a-a5367a47118b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 148.8211154937744, \"sum\": 148.8211154937744, \"min\": 148.8211154937744}}, \"EndTime\": 1613068499.602778, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068499.453474}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:01 INFO 139671722272576] Epoch[17] Batch[0] avg_epoch_loss=0.940880\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:01 INFO 139671722272576] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=0.940880358219\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:03 INFO 139671722272576] Epoch[17] Batch[5] avg_epoch_loss=0.587783\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:03 INFO 139671722272576] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=0.587782859802\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:03 INFO 139671722272576] Epoch[17] Batch [5]#011Speed: 129.02 samples/sec#011loss=0.587783\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:06 INFO 139671722272576] Epoch[17] Batch[10] avg_epoch_loss=0.610377\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:06 INFO 139671722272576] #quality_metric: host=algo-1, epoch=17, batch=10 train loss <loss>=0.637489074469\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:06 INFO 139671722272576] Epoch[17] Batch [10]#011Speed: 134.66 samples/sec#011loss=0.637489\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:06 INFO 139671722272576] processed a total of 683 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 6451.267957687378, \"sum\": 6451.267957687378, \"min\": 6451.267957687378}}, \"EndTime\": 1613068506.054181, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068499.602845}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:06 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=105.868739465 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:06 INFO 139671722272576] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:06 INFO 139671722272576] #quality_metric: host=algo-1, epoch=17, train loss <loss>=0.610376593742\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:06 INFO 139671722272576] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:06 INFO 139671722272576] Saved checkpoint to \"/opt/ml/model/state_f2ceda9d-9c5f-4847-a318-8a6fe4cbac43-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 133.34298133850098, \"sum\": 133.34298133850098, \"min\": 133.34298133850098}}, \"EndTime\": 1613068506.188165, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068506.054259}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:07 INFO 139671722272576] Epoch[18] Batch[0] avg_epoch_loss=0.640123\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:07 INFO 139671722272576] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=0.640122532845\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:09 INFO 139671722272576] Epoch[18] Batch[5] avg_epoch_loss=0.566671\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:09 INFO 139671722272576] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=0.566671301921\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:09 INFO 139671722272576] Epoch[18] Batch [5]#011Speed: 147.81 samples/sec#011loss=0.566671\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:12 INFO 139671722272576] Epoch[18] Batch[10] avg_epoch_loss=0.547586\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:12 INFO 139671722272576] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=0.524684589356\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:12 INFO 139671722272576] Epoch[18] Batch [10]#011Speed: 150.21 samples/sec#011loss=0.524685\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:12 INFO 139671722272576] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5876.255989074707, \"sum\": 5876.255989074707, \"min\": 5876.255989074707}}, \"EndTime\": 1613068512.064563, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068506.188244}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:12 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=111.633543366 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:12 INFO 139671722272576] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:12 INFO 139671722272576] #quality_metric: host=algo-1, epoch=18, train loss <loss>=0.547586432573\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:12 INFO 139671722272576] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:12 INFO 139671722272576] Saved checkpoint to \"/opt/ml/model/state_c8a8f4c0-c925-4033-a536-1fae2ce72f20-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 124.7560977935791, \"sum\": 124.7560977935791, \"min\": 124.7560977935791}}, \"EndTime\": 1613068512.189874, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068512.064643}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:13 INFO 139671722272576] Epoch[19] Batch[0] avg_epoch_loss=0.298435\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:13 INFO 139671722272576] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=0.298434853554\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:15 INFO 139671722272576] Epoch[19] Batch[5] avg_epoch_loss=0.462300\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:15 INFO 139671722272576] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=0.462299630046\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:15 INFO 139671722272576] Epoch[19] Batch [5]#011Speed: 154.06 samples/sec#011loss=0.462300\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:17 INFO 139671722272576] processed a total of 598 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5335.819959640503, \"sum\": 5335.819959640503, \"min\": 5335.819959640503}}, \"EndTime\": 1613068517.525799, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068512.189924}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:17 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=112.07024814 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:17 INFO 139671722272576] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:17 INFO 139671722272576] #quality_metric: host=algo-1, epoch=19, train loss <loss>=0.335330957733\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:17 INFO 139671722272576] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:17 INFO 139671722272576] Saved checkpoint to \"/opt/ml/model/state_29336f72-88dd-46dc-8594-fc90011cb76c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 127.64120101928711, \"sum\": 127.64120101928711, \"min\": 127.64120101928711}}, \"EndTime\": 1613068517.654039, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068517.525882}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:19 INFO 139671722272576] Epoch[20] Batch[0] avg_epoch_loss=0.372878\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:19 INFO 139671722272576] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=0.372878313065\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:21 INFO 139671722272576] Epoch[20] Batch[5] avg_epoch_loss=0.292073\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:21 INFO 139671722272576] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=0.292072715859\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:21 INFO 139671722272576] Epoch[20] Batch [5]#011Speed: 153.09 samples/sec#011loss=0.292073\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:23 INFO 139671722272576] Epoch[20] Batch[10] avg_epoch_loss=0.352010\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:23 INFO 139671722272576] #quality_metric: host=algo-1, epoch=20, batch=10 train loss <loss>=0.423933942243\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:23 INFO 139671722272576] Epoch[20] Batch [10]#011Speed: 152.19 samples/sec#011loss=0.423934\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:23 INFO 139671722272576] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5786.425828933716, \"sum\": 5786.425828933716, \"min\": 5786.425828933716}}, \"EndTime\": 1613068523.440604, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068517.654121}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:23 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=114.749078583 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:23 INFO 139671722272576] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:23 INFO 139671722272576] #quality_metric: host=algo-1, epoch=20, train loss <loss>=0.352009636943\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:23 INFO 139671722272576] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:25 INFO 139671722272576] Epoch[21] Batch[0] avg_epoch_loss=0.252825\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:25 INFO 139671722272576] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=0.252824962139\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:27 INFO 139671722272576] Epoch[21] Batch[5] avg_epoch_loss=0.182395\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:27 INFO 139671722272576] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=0.182394791394\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:27 INFO 139671722272576] Epoch[21] Batch [5]#011Speed: 155.39 samples/sec#011loss=0.182395\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:28 INFO 139671722272576] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5350.8758544921875, \"sum\": 5350.8758544921875, \"min\": 5350.8758544921875}}, \"EndTime\": 1613068528.792013, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068523.440681}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:28 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=116.987566045 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:28 INFO 139671722272576] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:28 INFO 139671722272576] #quality_metric: host=algo-1, epoch=21, train loss <loss>=0.196164613217\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:28 INFO 139671722272576] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:28 INFO 139671722272576] Saved checkpoint to \"/opt/ml/model/state_66730177-38ba-458c-80c7-d0798b7ed1df-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 125.0, \"sum\": 125.0, \"min\": 125.0}}, \"EndTime\": 1613068528.917623, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068528.792096}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:30 INFO 139671722272576] Epoch[22] Batch[0] avg_epoch_loss=0.472725\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:30 INFO 139671722272576] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=0.472724765539\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:32 INFO 139671722272576] Epoch[22] Batch[5] avg_epoch_loss=0.120443\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:32 INFO 139671722272576] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=0.120443121685\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:32 INFO 139671722272576] Epoch[22] Batch [5]#011Speed: 155.92 samples/sec#011loss=0.120443\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:34 INFO 139671722272576] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5325.217962265015, \"sum\": 5325.217962265015, \"min\": 5325.217962265015}}, \"EndTime\": 1613068534.242961, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068528.917676}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:34 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=116.987834803 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:34 INFO 139671722272576] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:34 INFO 139671722272576] #quality_metric: host=algo-1, epoch=22, train loss <loss>=0.0282935998403\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:34 INFO 139671722272576] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:34 INFO 139671722272576] Saved checkpoint to \"/opt/ml/model/state_6db3e18c-1792-4917-9724-a7f1ce13dca3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 127.03990936279297, \"sum\": 127.03990936279297, \"min\": 127.03990936279297}}, \"EndTime\": 1613068534.370611, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068534.243044}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:35 INFO 139671722272576] Epoch[23] Batch[0] avg_epoch_loss=0.478903\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:35 INFO 139671722272576] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=0.478902667761\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:37 INFO 139671722272576] Epoch[23] Batch[5] avg_epoch_loss=0.001144\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:37 INFO 139671722272576] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=0.00114395158986\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:37 INFO 139671722272576] Epoch[23] Batch [5]#011Speed: 157.86 samples/sec#011loss=0.001144\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:39 INFO 139671722272576] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5272.575855255127, \"sum\": 5272.575855255127, \"min\": 5272.575855255127}}, \"EndTime\": 1613068539.643339, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068534.370682}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:39 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=118.345550494 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:39 INFO 139671722272576] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:39 INFO 139671722272576] #quality_metric: host=algo-1, epoch=23, train loss <loss>=0.000926001742482\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:39 INFO 139671722272576] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:39 INFO 139671722272576] Saved checkpoint to \"/opt/ml/model/state_74287ab7-9c39-4763-9927-d8b0781867b8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 125.08296966552734, \"sum\": 125.08296966552734, \"min\": 125.08296966552734}}, \"EndTime\": 1613068539.769017, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068539.643422}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:41 INFO 139671722272576] Epoch[24] Batch[0] avg_epoch_loss=-0.141606\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:41 INFO 139671722272576] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=-0.141605988145\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:43 INFO 139671722272576] Epoch[24] Batch[5] avg_epoch_loss=0.156440\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:43 INFO 139671722272576] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=0.15643950055\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:43 INFO 139671722272576] Epoch[24] Batch [5]#011Speed: 155.71 samples/sec#011loss=0.156440\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:45 INFO 139671722272576] Epoch[24] Batch[10] avg_epoch_loss=-0.186816\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:45 INFO 139671722272576] #quality_metric: host=algo-1, epoch=24, batch=10 train loss <loss>=-0.598722750694\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:45 INFO 139671722272576] Epoch[24] Batch [10]#011Speed: 147.20 samples/sec#011loss=-0.598723\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:45 INFO 139671722272576] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5815.781116485596, \"sum\": 5815.781116485596, \"min\": 5815.781116485596}}, \"EndTime\": 1613068545.584937, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068539.769092}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:45 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=115.54537527 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:45 INFO 139671722272576] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:45 INFO 139671722272576] #quality_metric: host=algo-1, epoch=24, train loss <loss>=-0.186816068197\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:45 INFO 139671722272576] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:45 INFO 139671722272576] Saved checkpoint to \"/opt/ml/model/state_5b8a8ed2-6eb2-43f2-a7bb-2fe4b093ec0b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 136.83605194091797, \"sum\": 136.83605194091797, \"min\": 136.83605194091797}}, \"EndTime\": 1613068545.722321, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068545.585016}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:47 INFO 139671722272576] Epoch[25] Batch[0] avg_epoch_loss=0.033211\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:47 INFO 139671722272576] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=0.0332111679018\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:49 INFO 139671722272576] Epoch[25] Batch[5] avg_epoch_loss=-0.007525\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:49 INFO 139671722272576] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=-0.00752467724184\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:49 INFO 139671722272576] Epoch[25] Batch [5]#011Speed: 154.29 samples/sec#011loss=-0.007525\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:51 INFO 139671722272576] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5313.881158828735, \"sum\": 5313.881158828735, \"min\": 5313.881158828735}}, \"EndTime\": 1613068551.036343, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068545.722395}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:51 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=119.307422504 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:51 INFO 139671722272576] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:51 INFO 139671722272576] #quality_metric: host=algo-1, epoch=25, train loss <loss>=-0.0274895168841\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:51 INFO 139671722272576] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:52 INFO 139671722272576] Epoch[26] Batch[0] avg_epoch_loss=-0.150854\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:52 INFO 139671722272576] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=-0.150853738189\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:54 INFO 139671722272576] Epoch[26] Batch[5] avg_epoch_loss=-0.256471\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:54 INFO 139671722272576] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=-0.256471390525\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:54 INFO 139671722272576] Epoch[26] Batch [5]#011Speed: 156.28 samples/sec#011loss=-0.256471\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:56 INFO 139671722272576] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5322.0319747924805, \"sum\": 5322.0319747924805, \"min\": 5322.0319747924805}}, \"EndTime\": 1613068556.358933, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068551.036427}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:56 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=119.124677907 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:56 INFO 139671722272576] #progress_metric: host=algo-1, completed 67 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:56 INFO 139671722272576] #quality_metric: host=algo-1, epoch=26, train loss <loss>=-0.270174208283\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:56 INFO 139671722272576] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:56 INFO 139671722272576] Saved checkpoint to \"/opt/ml/model/state_92d9fd57-81fc-41c2-b290-bcbf08248d0c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 120.91207504272461, \"sum\": 120.91207504272461, \"min\": 120.91207504272461}}, \"EndTime\": 1613068556.480456, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068556.359017}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:58 INFO 139671722272576] Epoch[27] Batch[0] avg_epoch_loss=-0.118988\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:35:58 INFO 139671722272576] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=-0.11898817122\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:00 INFO 139671722272576] Epoch[27] Batch[5] avg_epoch_loss=-0.211966\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:00 INFO 139671722272576] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=-0.211965582023\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:00 INFO 139671722272576] Epoch[27] Batch [5]#011Speed: 154.33 samples/sec#011loss=-0.211966\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:01 INFO 139671722272576] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5303.2519817352295, \"sum\": 5303.2519817352295, \"min\": 5303.2519817352295}}, \"EndTime\": 1613068561.783841, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068556.480532}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:01 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=119.735160359 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:01 INFO 139671722272576] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:01 INFO 139671722272576] #quality_metric: host=algo-1, epoch=27, train loss <loss>=-0.248772037774\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:01 INFO 139671722272576] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:03 INFO 139671722272576] Epoch[28] Batch[0] avg_epoch_loss=-0.095597\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:03 INFO 139671722272576] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=-0.0955973193049\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:05 INFO 139671722272576] Epoch[28] Batch[5] avg_epoch_loss=-0.222683\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:05 INFO 139671722272576] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=-0.222683055947\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:05 INFO 139671722272576] Epoch[28] Batch [5]#011Speed: 128.70 samples/sec#011loss=-0.222683\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:07 INFO 139671722272576] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5800.214052200317, \"sum\": 5800.214052200317, \"min\": 5800.214052200317}}, \"EndTime\": 1613068567.584587, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068561.783923}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:07 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=108.786821377 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:07 INFO 139671722272576] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:07 INFO 139671722272576] #quality_metric: host=algo-1, epoch=28, train loss <loss>=-0.311054187268\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:07 INFO 139671722272576] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:07 INFO 139671722272576] Saved checkpoint to \"/opt/ml/model/state_3c72b2a5-454f-475e-8d8d-ac42d07c6bc4-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 126.08695030212402, \"sum\": 126.08695030212402, \"min\": 126.08695030212402}}, \"EndTime\": 1613068567.711292, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068567.58467}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:09 INFO 139671722272576] Epoch[29] Batch[0] avg_epoch_loss=-0.274421\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:09 INFO 139671722272576] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=-0.274420887232\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:11 INFO 139671722272576] Epoch[29] Batch[5] avg_epoch_loss=-0.416581\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:11 INFO 139671722272576] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=-0.416580503186\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:11 INFO 139671722272576] Epoch[29] Batch [5]#011Speed: 154.01 samples/sec#011loss=-0.416581\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:13 INFO 139671722272576] Epoch[29] Batch[10] avg_epoch_loss=-0.476386\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:13 INFO 139671722272576] #quality_metric: host=algo-1, epoch=29, batch=10 train loss <loss>=-0.548152315617\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:13 INFO 139671722272576] Epoch[29] Batch [10]#011Speed: 151.24 samples/sec#011loss=-0.548152\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:13 INFO 139671722272576] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5787.2960567474365, \"sum\": 5787.2960567474365, \"min\": 5787.2960567474365}}, \"EndTime\": 1613068573.49874, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068567.711358}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:13 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=113.867349727 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:13 INFO 139671722272576] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:13 INFO 139671722272576] #quality_metric: host=algo-1, epoch=29, train loss <loss>=-0.476385872472\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:13 INFO 139671722272576] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:13 INFO 139671722272576] Saved checkpoint to \"/opt/ml/model/state_7561816c-3b79-4a85-aa79-8e5259338a26-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 124.24492835998535, \"sum\": 124.24492835998535, \"min\": 124.24492835998535}}, \"EndTime\": 1613068573.623553, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068573.498819}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:15 INFO 139671722272576] Epoch[30] Batch[0] avg_epoch_loss=-0.092196\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:15 INFO 139671722272576] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=-0.0921959280968\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:17 INFO 139671722272576] Epoch[30] Batch[5] avg_epoch_loss=-0.265140\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:17 INFO 139671722272576] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=-0.26513988773\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:17 INFO 139671722272576] Epoch[30] Batch [5]#011Speed: 155.88 samples/sec#011loss=-0.265140\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:19 INFO 139671722272576] Epoch[30] Batch[10] avg_epoch_loss=-0.337420\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:19 INFO 139671722272576] #quality_metric: host=algo-1, epoch=30, batch=10 train loss <loss>=-0.424156200886\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:19 INFO 139671722272576] Epoch[30] Batch [10]#011Speed: 151.89 samples/sec#011loss=-0.424156\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:19 INFO 139671722272576] processed a total of 695 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5797.708988189697, \"sum\": 5797.708988189697, \"min\": 5797.708988189697}}, \"EndTime\": 1613068579.421408, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068573.623628}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:19 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=119.872393429 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:19 INFO 139671722272576] #progress_metric: host=algo-1, completed 77 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:19 INFO 139671722272576] #quality_metric: host=algo-1, epoch=30, train loss <loss>=-0.337420030074\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:19 INFO 139671722272576] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:20 INFO 139671722272576] Epoch[31] Batch[0] avg_epoch_loss=-0.436048\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:20 INFO 139671722272576] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=-0.436047613621\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:23 INFO 139671722272576] Epoch[31] Batch[5] avg_epoch_loss=-0.451422\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:23 INFO 139671722272576] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=-0.451422100266\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:23 INFO 139671722272576] Epoch[31] Batch [5]#011Speed: 156.86 samples/sec#011loss=-0.451422\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:25 INFO 139671722272576] Epoch[31] Batch[10] avg_epoch_loss=-0.526876\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:25 INFO 139671722272576] #quality_metric: host=algo-1, epoch=31, batch=10 train loss <loss>=-0.617419672012\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:25 INFO 139671722272576] Epoch[31] Batch [10]#011Speed: 151.92 samples/sec#011loss=-0.617420\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:25 INFO 139671722272576] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5723.3240604400635, \"sum\": 5723.3240604400635, \"min\": 5723.3240604400635}}, \"EndTime\": 1613068585.145293, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068579.42149}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:25 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=113.393482197 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:25 INFO 139671722272576] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:25 INFO 139671722272576] #quality_metric: host=algo-1, epoch=31, train loss <loss>=-0.526875541969\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:25 INFO 139671722272576] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:25 INFO 139671722272576] Saved checkpoint to \"/opt/ml/model/state_d5b72a44-54a2-4dc5-a715-1d61ccae779c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 180.32097816467285, \"sum\": 180.32097816467285, \"min\": 180.32097816467285}}, \"EndTime\": 1613068585.326178, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068585.145368}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:26 INFO 139671722272576] Epoch[32] Batch[0] avg_epoch_loss=-0.167511\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:26 INFO 139671722272576] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=-0.167511329055\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:28 INFO 139671722272576] Epoch[32] Batch[5] avg_epoch_loss=-0.260040\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:28 INFO 139671722272576] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=-0.260039944202\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:28 INFO 139671722272576] Epoch[32] Batch [5]#011Speed: 156.85 samples/sec#011loss=-0.260040\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:31 INFO 139671722272576] Epoch[32] Batch[10] avg_epoch_loss=-0.430936\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:31 INFO 139671722272576] #quality_metric: host=algo-1, epoch=32, batch=10 train loss <loss>=-0.636012226343\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:31 INFO 139671722272576] Epoch[32] Batch [10]#011Speed: 148.28 samples/sec#011loss=-0.636012\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:31 INFO 139671722272576] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5827.614068984985, \"sum\": 5827.614068984985, \"min\": 5827.614068984985}}, \"EndTime\": 1613068591.153933, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068585.326256}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:31 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=111.364135408 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:31 INFO 139671722272576] #progress_metric: host=algo-1, completed 82 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:31 INFO 139671722272576] #quality_metric: host=algo-1, epoch=32, train loss <loss>=-0.430936436084\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:31 INFO 139671722272576] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:32 INFO 139671722272576] Epoch[33] Batch[0] avg_epoch_loss=-0.243791\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:32 INFO 139671722272576] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=-0.243791222572\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:34 INFO 139671722272576] Epoch[33] Batch[5] avg_epoch_loss=-0.445748\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:34 INFO 139671722272576] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=-0.445747589072\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:34 INFO 139671722272576] Epoch[33] Batch [5]#011Speed: 152.43 samples/sec#011loss=-0.445748\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:36 INFO 139671722272576] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5377.964973449707, \"sum\": 5377.964973449707, \"min\": 5377.964973449707}}, \"EndTime\": 1613068596.53244, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068591.154012}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:36 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=115.282028566 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:36 INFO 139671722272576] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:36 INFO 139671722272576] #quality_metric: host=algo-1, epoch=33, train loss <loss>=-0.420841997862\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:36 INFO 139671722272576] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:38 INFO 139671722272576] Epoch[34] Batch[0] avg_epoch_loss=-0.353394\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:38 INFO 139671722272576] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=-0.353394299746\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:40 INFO 139671722272576] Epoch[34] Batch[5] avg_epoch_loss=-0.420532\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:40 INFO 139671722272576] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=-0.420532315969\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:40 INFO 139671722272576] Epoch[34] Batch [5]#011Speed: 153.71 samples/sec#011loss=-0.420532\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:41 INFO 139671722272576] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5340.259075164795, \"sum\": 5340.259075164795, \"min\": 5340.259075164795}}, \"EndTime\": 1613068601.873291, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068596.532525}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:41 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=119.279945073 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:41 INFO 139671722272576] #progress_metric: host=algo-1, completed 87 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:41 INFO 139671722272576] #quality_metric: host=algo-1, epoch=34, train loss <loss>=-0.444639539719\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:41 INFO 139671722272576] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:43 INFO 139671722272576] Epoch[35] Batch[0] avg_epoch_loss=-0.597372\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:43 INFO 139671722272576] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=-0.597372055054\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:45 INFO 139671722272576] Epoch[35] Batch[5] avg_epoch_loss=-0.617884\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:45 INFO 139671722272576] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=-0.617883755515\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:45 INFO 139671722272576] Epoch[35] Batch [5]#011Speed: 154.81 samples/sec#011loss=-0.617884\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:47 INFO 139671722272576] Epoch[35] Batch[10] avg_epoch_loss=-0.644487\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:47 INFO 139671722272576] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=-0.676409906149\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:47 INFO 139671722272576] Epoch[35] Batch [10]#011Speed: 153.62 samples/sec#011loss=-0.676410\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:47 INFO 139671722272576] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5758.136034011841, \"sum\": 5758.136034011841, \"min\": 5758.136034011841}}, \"EndTime\": 1613068607.632006, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068601.873373}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:47 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=116.528439907 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:47 INFO 139671722272576] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:47 INFO 139671722272576] #quality_metric: host=algo-1, epoch=35, train loss <loss>=-0.644486551258\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:47 INFO 139671722272576] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:47 INFO 139671722272576] Saved checkpoint to \"/opt/ml/model/state_f51e9c9d-39d4-4f91-bf1b-6841023c1b91-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 115.53001403808594, \"sum\": 115.53001403808594, \"min\": 115.53001403808594}}, \"EndTime\": 1613068607.748118, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068607.632082}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:49 INFO 139671722272576] Epoch[36] Batch[0] avg_epoch_loss=-0.712034\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:49 INFO 139671722272576] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=-0.71203404665\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:51 INFO 139671722272576] Epoch[36] Batch[5] avg_epoch_loss=-0.704193\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:51 INFO 139671722272576] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=-0.704193045696\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:51 INFO 139671722272576] Epoch[36] Batch [5]#011Speed: 153.27 samples/sec#011loss=-0.704193\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:53 INFO 139671722272576] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5330.652952194214, \"sum\": 5330.652952194214, \"min\": 5330.652952194214}}, \"EndTime\": 1613068613.078916, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068607.748197}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:53 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=120.057611097 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:53 INFO 139671722272576] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:53 INFO 139671722272576] #quality_metric: host=algo-1, epoch=36, train loss <loss>=-0.632202219963\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:53 INFO 139671722272576] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:54 INFO 139671722272576] Epoch[37] Batch[0] avg_epoch_loss=-0.344248\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:54 INFO 139671722272576] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=-0.344248384237\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:56 INFO 139671722272576] Epoch[37] Batch[5] avg_epoch_loss=-0.628817\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:56 INFO 139671722272576] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=-0.62881706655\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:56 INFO 139671722272576] Epoch[37] Batch [5]#011Speed: 154.42 samples/sec#011loss=-0.628817\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:58 INFO 139671722272576] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5293.047904968262, \"sum\": 5293.047904968262, \"min\": 5293.047904968262}}, \"EndTime\": 1613068618.372516, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068613.078997}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:58 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=119.021396507 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:58 INFO 139671722272576] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:58 INFO 139671722272576] #quality_metric: host=algo-1, epoch=37, train loss <loss>=-0.652465933561\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:58 INFO 139671722272576] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:36:58 INFO 139671722272576] Saved checkpoint to \"/opt/ml/model/state_93e8eb62-29cd-46d3-8db9-689a73fa69a2-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 120.66006660461426, \"sum\": 120.66006660461426, \"min\": 120.66006660461426}}, \"EndTime\": 1613068618.493809, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068618.372597}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:00 INFO 139671722272576] Epoch[38] Batch[0] avg_epoch_loss=-1.073480\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:00 INFO 139671722272576] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=-1.07348012924\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:02 INFO 139671722272576] Epoch[38] Batch[5] avg_epoch_loss=-0.813500\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:02 INFO 139671722272576] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=-0.813500235478\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:02 INFO 139671722272576] Epoch[38] Batch [5]#011Speed: 145.78 samples/sec#011loss=-0.813500\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:04 INFO 139671722272576] Epoch[38] Batch[10] avg_epoch_loss=-0.464945\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:04 INFO 139671722272576] #quality_metric: host=algo-1, epoch=38, batch=10 train loss <loss>=-0.0466795623302\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:04 INFO 139671722272576] Epoch[38] Batch [10]#011Speed: 123.03 samples/sec#011loss=-0.046680\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:04 INFO 139671722272576] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 6416.758060455322, \"sum\": 6416.758060455322, \"min\": 6416.758060455322}}, \"EndTime\": 1613068624.910745, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068618.49388}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:04 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=102.541487017 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:04 INFO 139671722272576] #progress_metric: host=algo-1, completed 97 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:04 INFO 139671722272576] #quality_metric: host=algo-1, epoch=38, train loss <loss>=-0.464945384047\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:04 INFO 139671722272576] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:06 INFO 139671722272576] Epoch[39] Batch[0] avg_epoch_loss=-0.689185\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:06 INFO 139671722272576] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=-0.689184844494\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:08 INFO 139671722272576] Epoch[39] Batch[5] avg_epoch_loss=-0.584060\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:08 INFO 139671722272576] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=-0.584059896568\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:08 INFO 139671722272576] Epoch[39] Batch [5]#011Speed: 157.75 samples/sec#011loss=-0.584060\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:10 INFO 139671722272576] processed a total of 590 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5300.029039382935, \"sum\": 5300.029039382935, \"min\": 5300.029039382935}}, \"EndTime\": 1613068630.211349, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068624.910825}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:10 INFO 139671722272576] #throughput_metric: host=algo-1, train throughput=111.31758592 records/second\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:10 INFO 139671722272576] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:10 INFO 139671722272576] #quality_metric: host=algo-1, epoch=39, train loss <loss>=-0.741843570769\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:10 INFO 139671722272576] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:10 INFO 139671722272576] Saved checkpoint to \"/opt/ml/model/state_aa6b86e8-1b84-43e7-8958-fb9e42a27f92-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 117.82717704772949, \"sum\": 117.82717704772949, \"min\": 117.82717704772949}}, \"EndTime\": 1613068630.329811, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068630.211432}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:10 INFO 139671722272576] Final loss: -0.741843570769 (occurred at epoch 39)\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:10 INFO 139671722272576] #quality_metric: host=algo-1, train final_loss <loss>=-0.741843570769\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:10 WARNING 139671722272576] You are using large values for `context_length` and/or `prediction_length`. The following step may take some time. If the step crashes, use an instance with more memory or reduce these two parameters.\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:10 INFO 139671722272576] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:10 WARNING 139671722272576] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:10 INFO 139671722272576] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 18203.208208084106, \"sum\": 18203.208208084106, \"min\": 18203.208208084106}}, \"EndTime\": 1613068648.533701, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068630.329865}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:29 INFO 139671722272576] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 18706.89296722412, \"sum\": 18706.89296722412, \"min\": 18706.89296722412}}, \"EndTime\": 1613068649.03734, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068648.533917}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:29 INFO 139671722272576] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:29 INFO 139671722272576] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 78.19604873657227, \"sum\": 78.19604873657227, \"min\": 78.19604873657227}}, \"EndTime\": 1613068649.115631, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068649.037395}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:29 INFO 139671722272576] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:29 INFO 139671722272576] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.bind.time\": {\"count\": 1, \"max\": 0.029087066650390625, \"sum\": 0.029087066650390625, \"min\": 0.029087066650390625}}, \"EndTime\": 1613068649.116352, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068649.115686}\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python2.7/site-packages/numpy/ma/core.py:2785: UserWarning: Warning: converting a masked element to nan.\n",
      "  order=order, subok=True, ndmin=ndmin)\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python2.7/site-packages/algorithm/evaluation.py:150: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return metrics[metric] / metrics[\"abs_target_sum\"]\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.score.time\": {\"count\": 1, \"max\": 6484.980821609497, \"sum\": 6484.980821609497, \"min\": 6484.980821609497}}, \"EndTime\": 1613068655.601307, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068649.116393}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:35 INFO 139671722272576] #test_score (algo-1, RMSE): 0.0468677487706\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:35 INFO 139671722272576] #test_score (algo-1, mean_absolute_QuantileLoss): 103.34867808275804\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:35 INFO 139671722272576] #test_score (algo-1, mean_wQuantileLoss): inf\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:35 INFO 139671722272576] #test_score (algo-1, wQuantileLoss[0.1]): inf\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:35 INFO 139671722272576] #test_score (algo-1, wQuantileLoss[0.2]): inf\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:35 INFO 139671722272576] #test_score (algo-1, wQuantileLoss[0.3]): inf\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:35 INFO 139671722272576] #test_score (algo-1, wQuantileLoss[0.4]): inf\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:35 INFO 139671722272576] #test_score (algo-1, wQuantileLoss[0.5]): inf\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:35 INFO 139671722272576] #test_score (algo-1, wQuantileLoss[0.6]): inf\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:35 INFO 139671722272576] #test_score (algo-1, wQuantileLoss[0.7]): inf\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:35 INFO 139671722272576] #test_score (algo-1, wQuantileLoss[0.8]): inf\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:35 INFO 139671722272576] #test_score (algo-1, wQuantileLoss[0.9]): inf\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:35 INFO 139671722272576] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=inf\u001b[0m\n",
      "\u001b[34m[02/11/2021 18:37:35 INFO 139671722272576] #quality_metric: host=algo-1, test RMSE <loss>=0.0468677487706\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 264959.83695983887, \"sum\": 264959.83695983887, \"min\": 264959.83695983887}, \"setuptime\": {\"count\": 1, \"max\": 9.399890899658203, \"sum\": 9.399890899658203, \"min\": 9.399890899658203}}, \"EndTime\": 1613068655.742027, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1613068655.601368}\n",
      "\u001b[0m\n",
      "\n",
      "2021-02-11 18:37:54 Uploading - Uploading generated training model\n",
      "2021-02-11 18:37:54 Completed - Training job completed\n",
      "Training seconds: 327\n",
      "Billable seconds: 327\n",
      "CPU times: user 1.19 s, sys: 64.3 ms, total: 1.25 s\n",
      "Wall time: 8min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_channels = {\n",
    "    \"train\": \"{}/train/\".format(s3_output_path),\n",
    "    \"test\": \"{}/test/\".format(s3_output_path)\n",
    "}\n",
    "\n",
    "estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you pass a test set in this example, accuracy metrics for the forecast are computed and logged (see bottom of the log).\n",
    "You can find the definition of these metrics from [our documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html). You can use these to optimize the parameters and tune your model or use SageMaker's [Automated Model Tuning service](https://aws.amazon.com/blogs/aws/sagemaker-automatic-model-tuning/) to tune the model for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint and predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained model, we can use it to perform predictions by deploying it to an endpoint.\n",
    "\n",
    "**Note: Remember to delete the endpoint after running this experiment. A cell at the very bottom of this notebook will do that: make sure you run it at the end.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To query the endpoint and perform predictions, we can define the following utility class: this allows making requests using `pandas.Series` objects rather than raw JSON strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import IdentitySerializer\n",
    "class DeepARPredictor(sagemaker.predictor.Predictor):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, \n",
    "                         serializer=IdentitySerializer(content_type=\"application/json\"),\n",
    "                         **kwargs)\n",
    "        \n",
    "    def predict(self, ts, cat=None, dynamic_feat=None, \n",
    "                num_samples=100, return_samples=False, quantiles=[\"0.1\", \"0.5\", \"0.9\"]):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "        \n",
    "        ts -- `pandas.Series` object, the time series to predict\n",
    "        cat -- integer, the group associated to the time series (default: None)\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        return_samples -- boolean indicating whether to include samples in the response (default: False)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "        \n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_time = ts.index[-1] + ts.index.freq\n",
    "\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, ts.index.freq, prediction_time, return_samples)\n",
    "    \n",
    "    def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\n",
    "        instance = series_to_dict(ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None)\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"] if return_samples else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles\n",
    "        }\n",
    "        \n",
    "        http_request_data = {\n",
    "            \"instances\": [instance],\n",
    "            \"configuration\": configuration\n",
    "        }\n",
    "        \n",
    "        return json.dumps(http_request_data).encode('utf-8')\n",
    "    \n",
    "    def __decode_response(self, response, freq, prediction_time, return_samples):\n",
    "        # we only sent one time series so we only receive one in return\n",
    "        # however, if possible one will pass multiple time series as predictions will then be faster\n",
    "        predictions = json.loads(response.decode('utf-8'))['predictions'][0]\n",
    "        prediction_length = len(next(iter(predictions['quantiles'].values())))\n",
    "        prediction_index = pd.date_range(start=prediction_time, freq=freq, periods=prediction_length)    \n",
    "        if return_samples:\n",
    "            dict_of_samples = {'sample_' + str(i): s for i, s in enumerate(predictions['samples'])}\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(data={**predictions['quantiles'], **dict_of_samples}, index=prediction_index)\n",
    "\n",
    "    def set_frequency(self, freq):\n",
    "        self.freq = freq\n",
    "        \n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]        \n",
    "\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat        \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can deploy the model and create and endpoint that can be queried using our custom DeepARPredictor class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    predictor_cls=DeepARPredictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the `predictor` object to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "respDF=predictor.predict(ts=timeseries[4], quantiles=[0.90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing predicted cost for next 10 hours  at 0.9 quantile. Please note, the data is generated using Device simulators and not recorded by actual sensors. Predicted cost can sometime display negative numbers, which will not be the case in case of real life scenario. Power company will never credit money for low consumption!!!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-02-10 23:00:00</th>\n",
       "      <td>473.603363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-11 00:00:00</th>\n",
       "      <td>1057.717285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-11 01:00:00</th>\n",
       "      <td>1162.437012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-11 02:00:00</th>\n",
       "      <td>1099.262207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-11 03:00:00</th>\n",
       "      <td>602.578064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-11 04:00:00</th>\n",
       "      <td>530.762756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-11 05:00:00</th>\n",
       "      <td>321.882507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-11 06:00:00</th>\n",
       "      <td>346.402832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-11 07:00:00</th>\n",
       "      <td>353.447937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-11 08:00:00</th>\n",
       "      <td>123.476425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0.9\n",
       "2021-02-10 23:00:00   473.603363\n",
       "2021-02-11 00:00:00  1057.717285\n",
       "2021-02-11 01:00:00  1162.437012\n",
       "2021-02-11 02:00:00  1099.262207\n",
       "2021-02-11 03:00:00   602.578064\n",
       "2021-02-11 04:00:00   530.762756\n",
       "2021-02-11 05:00:00   321.882507\n",
       "2021-02-11 06:00:00   346.402832\n",
       "2021-02-11 07:00:00   353.447937\n",
       "2021-02-11 08:00:00   123.476425"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respDF.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
